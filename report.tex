 \documentclass[conference]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{float}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
\usepackage{enumitem}
\begin{document}

\title{Overcoming Scanner Blindness: A Clean-Slate, Template-Driven DAST Framework (Case Study: OWASP Juice Shop)}

\author{\IEEEauthorblockN{Tudor Cristian Lacatus Cosma}}
\maketitle

\begin{abstract}
 The transition from Multi-Page Applications (MPAs) to Single Page Applications (SPAs) has rendered traditional crawling methodologies insufficient, creating a phenomenon known as "scanner blindness." Conventional Dynamic Application Security Testing (DAST) tools often fail to discover attack surfaces generated dynamically via client-side execution or background requests. This study proposes a template-driven DAST framework that combines Katana-based endpoint discovery (JavaScript crawling and XHR endpoint extraction) with YAML-defined active checks. Templates support endpoint placeholders (e.g., expanding across discovered API routes) and data-driven field-name auto-discovery patterns for common concepts such as privilege/role-like fields, tokens, and identifiers.

The system integrates an optional "lab mode" that provisions a fresh Docker container per scan run for reproducible, clean-slate experiments and to mitigate state pollution (e.g., database contamination) in targets designed to be resettable; lab profiles are defined via a lightweight YAML descriptor stored alongside the app templates. The solution is evaluated against OWASP Juice Shop as a representative SPA-backed application, focusing on business logic flaws and broken access control vectors commonly missed by signature-only engines. The results suggest that combining automated endpoint discovery, explicit authentication context, and specialized request templates improves practical coverage on modern JavaScript-heavy targets.
\end{abstract}

\begin{IEEEkeywords}
 DAST, Single Page Applications, Scanner Blindness, Idempotency, OWASP Juice Shop, State-Aware Scanning.
\end{IEEEkeywords}

\section{Introduction}

The evolution of web application development over the past two decades has been defined by a fundamental migration of logic, state, and rendering responsibilities from the server to the client. The industry has transitioned from the deterministic, synchronous nature of Server-Side Rendering (SSR) to the fluid, event-driven paradigm of Client-Side Rendering (CSR) and Single Page Applications (SPAs). While this architectural shift is often discussed in the context of Search Engine Optimization (SEO) challenges~\cite{Kowalczyk2024}, it has created a parallel and critical deficit in automated security testing.

Consequently, Dynamic Application Security Testing (DAST) tools, originally architected to crawl static HTML structures and inject payloads into synchronous HTTP forms, now face a significant coverage gap described in literature as "scanner blindness." This phenomenon occurs when scanners fail to identify attack surfaces that are generated dynamically via client-side execution. For example, legacy scanners rely on static \texttt{<a href="...">} tags to discover routes; however, modern SPAs handle navigation via JavaScript event listeners (e.g., \texttt{onclick} events that mutate the DOM). Without a JavaScript execution engine, a scanner cannot trigger these transitions, effectively leaving significant portions of the application untested~\cite{Doupe2012}. Modern frameworks, such as Angular, React, and Vue, present opaque surfaces to traditional tools, concealing vulnerabilities behind complex execution contexts and virtualized states.

To quantify the impact of these architectural shifts, the research community relies on empirical evaluations of tool effectiveness. Recent studies, such as those by Qadir et al. (2025), assess scanner performance by measuring the count and severity of detected vulnerabilities and mapping them to standardized industry benchmarks like the OWASP Top 10~\cite{Qadir2025}. These comparative frameworks are essential for identifying gaps in scanner coverage against real-world applications.

\subsection{Justification of Test Environment}
This project utilizes OWASP Juice Shop as the primary testing environment. While modern alternatives like "Broken Crystals" offer valuable benchmarks, they are often structured as collections of isolated, unrelated vulnerability test cases. In contrast, Juice Shop simulates a monolithic, interdependent e-commerce workflow where vulnerabilities are gated behind complex state transitions. In such environments, security testing frequently depends on preserving an authentication context (e.g., a bearer token) and executing request sequences in the correct order to satisfy preconditions (e.g., login before accessing protected APIs). This architectural cohesion makes Juice Shop a strong candidate for stress-testing a scanner’s ability to overcome "blindness" in deep navigational structures, while also highlighting the limits of purely stateless request fuzzing.

\subsection{Methodological Approach}
Unlike general-purpose tools such as OWASP ZAP or Burp Suite, which often operate against long-lived targets and can suffer from "state pollution" (where previous actions contaminate the database for subsequent tests), this solution supports an optional "Clean-Slate" methodology when the target can be provisioned as an ephemeral lab. Research by Arcuri on automated API testing emphasizes Resetting the System Under Test (SUT), specifically the database state, before test execution to ensure scientific validity~\cite{Arcuri2019}. The concrete lab-mode workflow and Docker integration are detailed in Section~4.1.

\subsection{Contributions}
This work makes the following practical contributions toward mitigating scanner blindness in JavaScript-heavy web applications:
\begin{itemize}
    \item \textbf{Clean-slate lab execution:} an optional lab mode that provisions an ephemeral Dockerized target per scan run to improve reproducibility and reduce state pollution during experiments.
    \item \textbf{Scaffolded app profiling:} an initialization workflow that discovers endpoints and generates an application profile to reduce the cost of onboarding new targets and authoring templates.
    \item \textbf{Template-driven, multi-step active checks:} YAML-defined request sequences with variable extraction/interpolation to express stateful workflows (e.g., authentication, token propagation) beyond single-request signatures.
    \item \textbf{Reproducible and extensible methodology:} a tiered template approach (generic baseline + app-specific overlays) intended to be easily reproduced and expanded to additional applications with minimal rework.
    \item \textbf{Standards-aligned reporting:} automatic mapping of findings to OWASP Top 10 (2025) categories to support executive-facing communication and benchmarking.
\end{itemize}

\subsection{Ethical Considerations and Safety}
All experiments were performed exclusively against deliberately vulnerable applications in a controlled local lab environment (OWASP Juice Shop), provisioned via Docker. Potentially destructive checks were executed only in the tool's clean-slate lab mode to isolate side effects and avoid persistent state pollution across runs. No scans were conducted against third-party systems, production deployments, or targets without explicit authorization.

\section{State of the Art}

\subsection{Dynamic Application Security Testing (DAST)}
Dynamic Application Security Testing (DAST) is a methodology for analyzing a web application in its running state, adopting the perspective of an external attacker. Unlike Static Application Security Testing (SAST), which operates as a "white-box" technique by inspecting source code, DAST operates as a "black-box" technique~\cite{Scarfone2008}. It possesses no prior knowledge of the internal architecture, logic flow, or server-side code; instead, it interacts with the application via the presentation layer (HTTP/HTTPS) to observe responses to induced stimuli.

While SAST acts as a code inspector, DAST acts as a behavioral analyst. This distinction is critical for identifying runtime vulnerabilities, such as misconfigurations or authentication logic flaws, that remain invisible during static code analysis.

\subsection{The Challenge of Single Page Applications (SPAs)}
The SPA shift, introduced in Section~1, turns discovery from deterministic link-following into stateful exploration. In the literature, the core difficulty is that reachable states are not enumerated a priori; they emerge from client-side execution and user-triggered transitions.

\subsubsection{The Crawling Deficit}
This architecture fundamentally disrupts the standard crawling model. A vulnerability (e.g., a broken "Delete User" endpoint) may only exist in a "Deep State" reachable solely via a specific sequence of DOM interactions: \textit{Login $\rightarrow$ List Users $\rightarrow$ Select User $\rightarrow$ Click Options}.

\begin{figure}[H]
    \centerline{\includegraphics[width=0.9\columnwidth]{1.png}}
    \caption{Navigation Paradigms: Single Page Applications (Top) update content dynamically without changing the URL context ($1 \rightarrow 1$), hiding states from traditional crawlers. Multi-Page Applications (Bottom) perform full navigations ($1 \rightarrow 2$) visible to static scanners.}
    \label{fig:blindness}
\end{figure}

Academic literature refers to this as the \textit{State Explosion Problem}. As noted by Doupé et al., traditional scanners that fail to execute JavaScript or track DOM mutations view SPAs as a single URL, missing the majority of the attack surface~\cite{Doupe2012}. This transforms crawling from a simple URL enumeration task into a complex Graph Traversal Problem, where nodes (states) are not pre-defined but must be inferred dynamically by transitions (edges)~\cite{Mesbah2012}.

\subsection{The DAST Operational Workflow}
While implementation details vary between tools, the archetypal DAST workflow follows a structured progression: Crawling $\rightarrow$ Fuzzing $\rightarrow$ Analysis.
H
\begin{figure}[H]
    \centerline{\includegraphics[width=0.9\columnwidth]{2.png}}
    \caption{The DAST Automation Pipeline: Discovery, Processing, and Reporting.}
    \label{fig:workflow}
\end{figure}

\subsubsection{Phase 1: Crawling (Discovery)}
The objective is to map the "Attack Surface." Modern scanners must move beyond passive HTML spidering to JavaScript-aware discovery.
\begin{itemize}
    \item \textbf{JavaScript-Aware Discovery:} Discovery can be augmented with JavaScript-capable crawlers (e.g., tools that parse and execute client-side code) to enumerate candidate URLs that are not present as static \texttt{href} links.
    \item \textbf{XHR Endpoint Collection:} Collecting endpoints associated with XHR-style traffic improves coverage on SPA backends where critical API paths are reached primarily through background requests.
    \item \textbf{Scope Note:} The approach studied here prioritizes backend endpoint discovery and template-driven HTTP checks; it does not attempt to fully reproduce UI-driven state transitions (e.g., multi-step click flows).
\end{itemize}

\subsubsection{Phase 2: Fuzzing vs. Scanning}
It is critical to distinguish between these terms, as they represent different depths of analysis.
\begin{itemize}
    \item \textbf{Vulnerability Scanning:} This is deterministic and signature-based. It checks for the presence of known attributes (e.g., "Is the Apache version outdated?" or "Is debug=true exposed?"). It asks: \textit{Is X present?}
    \item \textbf{Fuzzing:} This is non-deterministic and experimental. It involves generating invalid or unexpected data to observe how the application logic handles anomalies. For example, injecting \texttt{\{'name': 'John' OR '1'='1'\}} tests for SQL injection. It asks: \textit{What happens if I break the rules?}
\end{itemize}

\textbf{The Role of State in Fuzzing:}
Effective fuzzing requires context. A naive fuzzer fails because it sends payloads out of context (e.g., sending a checkout payload before authentication is established). The necessity of "State-Awareness" has been rigorously established in network security research. For instance, Natella et al. demonstrated that stateful fuzzers (those that infer and track the internal protocol state of a server) achieve significantly higher code coverage and bug detection rates than stateless fuzzers~\cite{natella}. While Natella's work focused on network protocols, the same principle applies to modern web applications: tests are more effective when they preserve an authentication context and execute prerequisite request sequences. In the current implementation, this state is represented primarily as a single authentication context (e.g., a bearer token) and an ordered list of template requests; the scanner does not execute full client-side UI state transitions.

\subsubsection{Phase 3: Analysis (The Oracle Problem)}
The final phase interprets the application's reaction. This confronts the "Oracle Problem": distinguishing between a secure failure (a handled 500 error) and an insecure failure (a crash, data leak, or successful bypass).


\section{Market Stratification and Tool Capabilities}

The landscape of Dynamic Application Security Testing (DAST) has undergone significant stratification. As web architectures have matured from server-side monoliths to complex client-side Single Page Applications (SPAs), the market has split into two distinct categories: widely adopted open-source frameworks driven by community contributions, and enterprise-grade commercial solutions characterized by advanced automation and proprietary crawling algorithms~\cite{Somi2024}.

\subsection{Global Market Dynamics}

The global market for application security is expanding, driven by the increasing complexity of web applications and the ubiquity of open-source components. Recent audits reveal that \textbf{81\% of commercial codebases contain high or critical-risk vulnerabilities}, necessitating robust automated testing~\cite{BlackDuck2025}. This expansion is further accelerated by the "transitive dependency" problem, where \textbf{64\% of open-source components are dependencies of dependencies}, making manual tracking nearly impossible and necessitating sophisticated scanning engines~\cite{BlackDuck2025}.

\subsection{Open-Source Ecosystem}

Open-source tools remain the backbone of the security research community, offering high customizability. However, they often face challenges regarding maintenance continuity and out-of-the-box efficacy against modern SPAs~\cite{Somi2024}.

\subsubsection{OWASP Zed Attack Proxy (ZAP)}
OWASP ZAP stands as the de-facto standard for open-source DAST. It is heavily favored for its seamless integration into CI/CD pipelines and ability to perform both active and passive scans~\cite{Somi2024}. Recent benchmarks indicate that ZAP performs consistently well against traditional vulnerabilities, achieving respectable recall rates (0.75–0.77) in controlled environments~\cite{Yarphel2025}. However, performance gaps exist. While ZAP can discover significantly more URLs in AJAX-heavy applications via its AJAX Spider compared to traditional crawlers~\cite{Chorell2024}, independent benchmarks note it can produce higher false positive rates in specific categories compared to commercial tools~\cite{Yarphel2025, Chorell2024}. Despite this, ZAP remains a viable open-source candidate due to its scriptability and wide adoption.

\subsubsection{ProjectDiscovery Nuclei}
A rapidly emerging tool is Nuclei, which shifts from heuristic scanning to template-based matching. Unlike ZAP, Nuclei often relies on external inputs (e.g., from tools like Katana) to map the attack surface rather than performing native crawling~\cite{Chorell2024}. Its architecture is built around YAML-based templates that define specific requests, allowing it to identify known vulnerabilities (CVEs) and misconfigurations.

In comparative studies, Nuclei demonstrated a \textbf{zero false-positive rate} on standard benchmarks like the OWASP Benchmark~\cite{Chorell2024}. However, this precision comes with a trade-off: the tool may fail to identify "unknown unknowns" if specific templates do not exist, effectively limiting its recall in scenarios requiring deep business logic interaction~\cite{Chorell2024}. Preliminary baseline tests conducted for this study against OWASP Juice Shop confirmed this limitation; while the Katana and Nuclei combination successfully identified static information leaks (e.g., Swagger definitions), it failed to detect business logic flaws such as "JWT None" algorithm confusion or negative basket quantities, largely due to the inability to maintain the necessary authentication state across request sequences. Furthermore, the default scanning intensity proved excessive for the target environment, triggering a Denial of Service (DoS) state that required manual intervention—highlighting the risks of generic high-concurrency fuzzing against fragile applications.

\subsubsection{Legacy and Niche Tools}
The open-source ecosystem also includes tools that have struggled to adapt to the SPA era.

\begin{itemize}
    \item \textbf{Wapiti:} A command-line based scanner that functions as a "black-box" fuzzer. While it boasts fast scan times, evaluations show it lacks a JavaScript engine capable of rendering AJAX-heavy applications~\cite{Chorell2024}. In tests against modern targets, Wapiti failed to discover input vectors hidden behind client-side dynamic rendering~\cite{Chorell2024}.
    \item \textbf{Arachni:} Once a high-performance framework, Arachni is now considered on the verge of obsolescence. Development has largely stalled, and commercial support has migrated to its successor, Codename SCNR~\cite{Chorell2024}.
\end{itemize}

\subsection{Commercial Landscape}

Commercial scanners distinguish themselves through "Smart Crawling" capabilities, proprietary engines designed to execute JavaScript and construct a complex state graph of the application.

\subsubsection{Burp Suite Professional}
Widely regarded as the industry gold standard, Burp Suite Professional combines automated scanning with a powerful manual testing toolkit~\cite{Somi2024}. Recent empirical comparisons rank it as the most effective overall scanner, achieving the highest precision (0.73) and recall (0.90) rates in controlled experiments~\cite{Yarphel2025}. Its primary strength lies in its advanced crawling engine, which can navigate complex client-side states and authentication sequences that often baffle open-source alternatives.

\subsubsection{Acunetix}
Acunetix focuses heavily on enterprise automation. It employs technology specifically engineered to crawl HTML5 and JavaScript-heavy applications. In comparative benchmarks, Acunetix (AWVS) demonstrated a more \textbf{conservative scanning strategy} compared to ZAP, generating fewer total HTTP requests while still maintaining competitive vulnerability detection rates~\cite{Yin2023}. This efficiency makes it a preferred choice for organizations prioritizing low network overhead.

\subsection{Comparative Summary}

The divergence between open-source and commercial tools is most visible in the "Crawling Deficit." Commercial tools like Burp Suite utilize mature state engines that require less manual configuration to achieve high coverage.

\section{Proposed Methodology}

\subsection{Architectural Design: The "Clean-Slate" Engine}
A primary failure mode in existing automated scanning is "State Pollution." When a scanner executes destructive payloads (e.g., deleting a user account or altering a database schema), it alters the System Under Test (SUT). Subsequent tests may then run against a corrupted state, leading to False Negatives (the vulnerability exists, but the precondition is broken) or False Positives (a crash occurs due to missing data, not a bug).

To solve this, the proposed scanner implements an \textbf{Idempotent Testing Architecture}.

\begin{itemize}
    \item \textbf{Integration:} The scanner integrates with Docker Engine via CLI automation.
    \item \textbf{Workflow:} In lab mode, the scanner provisions a fresh, ephemeral container per scan run.
    \item \textbf{Scientific Justification:} This approach enforces "Test Isolation" for reproducible experiments.
\end{itemize}

\begin{figure}[htbp]
    \makebox[\columnwidth][c]{\includegraphics[width=0.9\columnwidth]{3.png}}
    \caption{Visualizing State Pollution: Traditional scans (top) accumulate artifacts that may interfere with subsequent tests. The proposed Clean-Slate approach (bottom) resets the state, ensuring idempotency.}
    \label{fig:pollution}
\end{figure}

\subsection{Logic-Oriented Detection Modules}
The core differentiation of this tool is its focus on Business Logic Vulnerabilities (BLVs) and Broken Access Control (IDOR), expressed as explicit YAML templates. These categories have traditionally required human intuition because they do not necessarily trigger HTTP 500 crashes or obvious error messages. The framework operationalizes detection by executing parameterized request sequences (templates) against discovered endpoints, using matchers to recognize anomalous yet successful outcomes. This balances generality (generic templates) with target specificity (app templates), addressing practical "scanner blindness" by ensuring that modern API-heavy surfaces are actively exercised rather than inferred from static HTML alone~\cite{Doupe2012}.

In the current implementation, templates can be expanded over discovered endpoints using path macros such as \texttt{@all@} (expand over all cached endpoints, substituting each cached endpoint's URL path) and \texttt{@api@} (expand over cached endpoints using several modes). In numeric-suffix and fallback modes, \texttt{@api@} expands primarily over cached endpoints whose paths contain \texttt{/api/} or begin with \texttt{/api}; in named-suffix mode, suffix matching may also select non-\texttt{/api} endpoints depending on the cached endpoint paths. Templates can also expand payload field names using auto-discovery placeholders of the form \texttt{\{\{autodiscover:<type>\}\}} (e.g., \texttt{privilege}, \texttt{status}, \texttt{token}, \texttt{id}), where the candidate field-name lists are loaded from a JSON configuration. The current implementation expands at most one auto-discovery placeholder per request body.

\subsubsection{Business Logic: Semantic Validation (The "Negative Cost" Problem)}
Standard scanners rely heavily on HTTP status codes to determine pass/fail conditions. If an injection returns \texttt{500 Internal Server Error}, it flags a potential issue. If it returns \texttt{200 OK}, it assumes safety. This reliance on HTTP status is insufficient for logic flaws, where the server typically processes the request successfully but produces an invalid business outcome, a limitation highlighted in state-aware scanning research~\cite{Doupe2012, Yin2023}.

\textbf{The Flaw:} In the Juice Shop environment, it is possible to add products to the basket with a negative quantity (e.g., \texttt{quantity: -5}). The server processes this mathematically, resulting in a negative total price (effectively refunding the user). The HTTP response is often a valid \texttt{200 OK}.

\textbf{The Solution:} This check is implemented as an active template that injects anomalous numeric values (e.g., negative quantities) and flags cases where the server responds successfully to inputs that should be rejected by the business tier. The approach mirrors invariant-based testing and intent synchronization proposed by Yin et al. for enhancing vulnerability detection in modern web applications~\cite{Yin2023}.

\subsubsection{IDOR: Resource Deviation Strategy}
Insecure Direct Object References (IDOR) remain a "Blind Spot" for scanners because strong verification often benefits from comparing access patterns across different authorization contexts. The current implementation supports comparing unauthenticated execution against a single authenticated context (bearer token), but it does not implement multi-user differential testing (e.g., User A vs. User B sessions) as a first-class feature.

\textbf{The Solution:} The scanner implements a \textit{Template-Driven Resource Enumeration} strategy across discovered API endpoints, expressed as templates that expand over API paths and append candidate identifiers (e.g., \texttt{/1}). Verification remains matcher-driven: templates may use status checks as a lightweight triage signal, and can be strengthened with content matchers (e.g., patterns for PII fields) when target semantics are known. In practice, the framework supports running scans in both unauthenticated and authenticated modes to compare behaviors and highlight broken access control.

\begin{itemize}
    \item \textbf{Discovery:} The scanner maps endpoints and applies template-driven enumeration by appending candidate identifiers (e.g., \texttt{/1}) across discovered endpoints.
    \item \textbf{Attack Vector:} The scanner replays requests with modified identifiers (e.g., \texttt{GET /api/Basket/124}) while maintaining a bearer-token authentication context when provided.
    \item \textbf{Verification:} Templates apply matchers to identify suspiciously successful access (e.g., \texttt{200 OK}) and, when target semantics are known, may add content matchers for sensitive resource patterns (e.g., email, address, or transaction details).
\end{itemize}


\subsubsection{JWT Algorithm Confusion}

This targets JSON Web Token (JWT) algorithm confusion. A template helper can forge a JWT with \texttt{alg=none} by rewriting the header and stripping the signature (e.g., \texttt{jwt\_none(\{\{bearer\_token\}\})}). The forged token is replayed against endpoints requiring authentication to test for signature-verification bypass. While conceptually simple, this specific misconfiguration is frequently missed by generic fuzzers that do not parse the internal structure of JWTs. It represents a "Logic Bypass" where the server trusts the client-supplied directive to disable cryptographic verification, a class of vulnerability that contributes to the high-risk nature of open-source component dependencies~\cite{BlackDuck2025}.

\subsection{Operational Interface}
The framework is implemented as a Command Line Interface (CLI) application developed in Python. The operational workflow consists of two primary stages designed to enforce the separation between discovery and active exploitation.

\begin{itemize}
    \item \textbf{Initialization Phase:} This stage performs the "Discovery" tasks. It invokes the crawling engine to map the target's attack surface, caches the discovered endpoints (both static navigational links and XHR endpoints), and scaffolds the configuration descriptors. This prepares the workspace without sending active attack payloads.
    \item \textbf{Scanning Phase:} This stage executes the "Attack" logic. The engine loads the cached endpoints and iterates through the enabled vulnerability templates (both generic and app-specific). It supports selective execution of individual templates and configurable authentication contexts (e.g., authenticated vs. unauthenticated scans).
    \item \textbf{Reporting:} The system generates artifacts to aid analysis, including a structured JSON output for machine parsing and a rendered HTML report for human review.
\end{itemize}



\section{Experimental Evaluation}

To validate the efficacy of the proposed solution, a comparative benchmark was conducted against the "State of the Art" open-source toolchain: ProjectDiscovery's \textbf{Katana} (discovery) and \textbf{Nuclei} (scanning).

\subsection{Benchmark Setup}
The target environment was a locally provisioned instance of OWASP Juice Shop. Both scanners were configured with equivalent distinct phases:
\begin{enumerate}
    \item \textbf{Baseline (Control):} Katana (headless mode) fed into Nuclei (standard templates).
    \item \textbf{Experimental:} Proposed framework (Clean-Slate mode) with specific logic templates.
\end{enumerate}

\subsection{Results}
The results, summarized in Table~\ref{tab:comparison}, highlight the "Scanner Blindness" phenomenon. While the generic scanner successfully identified static misconfigurations (swagger.yaml exposure), it completely missed complex business logic flaws.

The generic scan also triggered a Denial of Service (DoS) state in the target due to excessive concurrency, whereas the proposed framework's controlled iteration remained stable.

\begin{table}[htbp]
    \caption{Detection Capability Comparison on OWASP Juice Shop}
    \begin{center}
    \begin{tabular}{|l|c|c|}
    \hline
    \textbf{Vulnerability Class} & \textbf{Nuclei (Generic)} & \textbf{Proposed Framework} \\
    \hline
    Info Disclosure (Swagger) & \textbf{Detected} & \textbf{Detected} \\
    \hline
    Broken Access (IDOR) & Missed & \textbf{Detected} \\
    \hline
    Logic Flaw (Neg. Qty) & Missed & \textbf{Detected} \\
    \hline
    JWT Algorithm Confusion & Missed & \textbf{Detected} \\
    \hline
    Target Stability & Unstable (DoS) & Stable \\
    \hline
    \end{tabular}
    \label{tab:comparison}
    \end{center}
\end{table}

\subsection{Analysis}
The failure of the generic toolchain to detect the logic flaws is attributed to its lack of state awareness. The IDOR and Negative Quantity vulnerabilities in Juice Shop require a valid authentication token and specific preconditions (e.g., a valid basket ID) which the generic templates did not maintain. The proposed framework's ability to interpolate captured tokens into active templates allowed it to reach the vulnerable code paths.

However, a code-level review reveals that this increased stability comes at the cost of execution speed. The proposed framework utilizes a sequential execution model (iterating through templates and expanding requests in serial \texttt{asyncio} loops) rather than a highly concurrent worker pool. While this design choice deliberately restricts request throughput—thereby preventing the Denial of Service (DoS) states observed with Nuclei's aggressive default concurrency—it naturally results in longer scan durations, particularly when templates expand across a large surface of discovered endpoints.

\bibliographystyle{IEEEtran}
\bibliography{sample}

\end{document}